{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import trange\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_table(mpileup):\n",
    "    frequency_table = {}\n",
    "    for i in mpileup.values:       \n",
    "        fastadict = {\"A\":0,\"T\":0,\"G\":0,\"C\":0}                    \n",
    "        sequence = i[4] #actual sequence                \n",
    "        sequence = sequence.upper() \n",
    "        \n",
    "        sequence = trimm_caret(sequence)                            \n",
    "        sequence = sequence.replace(\"$\", \"\")                   \n",
    "        indel_pos = find_all_indels(sequence)\n",
    "        ### Count number of indels\n",
    "        indels = count_indels(sequence, indel_pos)        \n",
    "        fastadict.update(indels)\n",
    "        fastadict[\"-\"] += sequence.count(\"*\")\n",
    "        ### Trimm Indels\n",
    "        trimm_sequence = trimm_indels(sequence, indel_pos)        \n",
    "        for seq in trimm_sequence:    \n",
    "            if seq in fastadict:            \n",
    "                fastadict[seq] +=1    \n",
    "        frequency_table.update({i[1]:list(fastadict.values())})    \n",
    "    df_frequency_table = pd.DataFrame.from_dict(frequency_table, orient='index')\n",
    "    df_frequency_table.columns = bases    \n",
    "    return df_frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bwa_mem(path_fastq_file, output_file, threads, ref_genome):        \n",
    "    cmd = \"bwa mem -t {} -B 1 -O 1 -E 1 -L 1 {} {} > {}\".format(threads, ref_genome, path_fastq_file, output_file)                    \n",
    "    try:        \n",
    "        subprocess.call(cmd, shell=True)\n",
    "        return True\n",
    "    except OSError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tmp_folder(tmp_folder):\n",
    "    if os.path.isdir(tmp_folder):            \n",
    "        cmd = 'rm -r {}'.format(tmp_folder)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        cmd = 'mkdir {}'.format(tmp_folder)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "    else:\n",
    "        cmd = 'mkdir {}'.format(tmp_folder)\n",
    "        subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_indels(s):\n",
    "    find_all = lambda c,s: [x for x in range(c.find(s), len(c)) if c[x] == s]\n",
    "    list_pos = []\n",
    "    for i in find_all(s,\"-\"):\n",
    "        list_pos.append(i)\n",
    "    for i in find_all(s,\"+\"):\n",
    "        list_pos.append(i)    \n",
    "    return sorted(list_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_indels(s, pos):    \n",
    "    dict_indel = {\"+\":0,\"-\":0}    \n",
    "    if pos == []:\n",
    "        return dict_indel            \n",
    "    if len(pos) > 0:\n",
    "        for i in range(0,len(pos)): \n",
    "            try: # in case it is not a number but a base pair e.g. A\n",
    "                dict_indel[s[pos[i]]] += int(s[pos[i]+1])                                                                        \n",
    "            except ValueError:                \n",
    "                dict_indel[s[pos[i]]] += 1\n",
    "                continue                \n",
    "    return dict_indel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimm_indels(s, pos):    \n",
    "    ## Receives a sequence and trimms indels            \n",
    "    if pos == []:\n",
    "        return s\n",
    "    u_sequence = \"\"  \n",
    "    start =  pos[0]\n",
    "    count = (start+1)    \n",
    "    try: # in case it is not a number but a base pair e.g. A\n",
    "        end = count+int(s[count])+1\n",
    "    except ValueError:                \n",
    "        end = start+1            \n",
    "    u_sequence = s[:start]    \n",
    "    if len(pos) > 1:\n",
    "        for i in range(1,len(pos)):                      \n",
    "            start = end                \n",
    "            u_sequence += s[start:pos[i]]\n",
    "            start = pos[i]\n",
    "            count = (start+1)            \n",
    "            try: # in case it is not a number but a base pair e.g. A\n",
    "                end = count+int(s[count])+1  \n",
    "            except ValueError:\n",
    "                end = start+1                \n",
    "            if pos[-1] == pos[i]:    \n",
    "                #print(s[end:])\n",
    "                u_sequence += s[end:]\n",
    "    else:        \n",
    "        u_sequence += s[end:]\n",
    "    return u_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimm_caret(s):        \n",
    "    find_all = lambda c,s: [x for x in range(c.find(s), len(c)) if c[x] == s]\n",
    "    list_pos = []\n",
    "    for i in find_all(s,\"^\"):\n",
    "        list_pos.append(i)    \n",
    "    if list_pos == []:\n",
    "        return s\n",
    "    i = 0    \n",
    "    start = 0\n",
    "    end = 0\n",
    "    sequence = \"\"\n",
    "    while i<len(s):\n",
    "        if s[i] == \"^\":        \n",
    "            end = i\n",
    "            sequence += (s[start:end])                    \n",
    "            start = i+1\n",
    "        elif i >= list_pos[-1]+1:\n",
    "            sequence += (s[list_pos[-1]+1:])\n",
    "            break\n",
    "        i+=1\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frequencies(dirpath):\n",
    "    \n",
    "    dirpath = os.walk(dirpath)\n",
    "    df_all = pd.DataFrame()\n",
    "    name_files = []\n",
    "    print(\"Reading files...\")\n",
    "    for dirpath, dirnames, filenames in dirpath:\n",
    "        for filename in [f for f in filenames if f.endswith(\".freq\")]:\n",
    "            path_freq_file = os.path.join(dirpath, filename)             \n",
    "            name_freq_file = path_freq_file.split('/')[-1]                  \n",
    "            name_files.append(name_freq_file)\n",
    "            df_cpgs_file = pd.read_table(path_freq_file, sep=\"\\t\", index_col=\"position\" )                                                            \n",
    "            df_all = df_all.append(df_cpgs_file.T)                                                         \n",
    "    df_all = df_all.fillna(0)\n",
    "    df_all = df_all.T            \n",
    "    print(\"Preprocessing dataframe...\")\n",
    "    print(len(name_files))\n",
    "    X_train = preprocess_df(df_all, name_files)    \n",
    "    print(\"Preprocessing Done!\")\n",
    "    \n",
    "    return X_train, name_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df_pos):\n",
    "    df = pd.DataFrame()\n",
    "    ### transoforms each sample to the sample scale row/all\n",
    "    for i in range(len(df_pos)):\n",
    "        row = df_pos.iloc[i]\n",
    "        row = row/sum(row)\n",
    "        df = df.append(pd.DataFrame(np.array(row).reshape(1,6), columns = df_pos.columns))\n",
    "    df = df.fillna(0)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "    df_transformed = scaler.transform(df)    \n",
    "    df_transformed = pd.DataFrame(df_transformed, columns=df_pos.columns, index=df_pos.index)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_all, name_files):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for index in range(len(df_all)):        \n",
    "        \n",
    "        df_tmp = pd.DataFrame()\n",
    "        a = pd.DataFrame(df_all.iloc[index][\"A\"])\n",
    "        a.index=name_files\n",
    "        a.columns = [\"A\"]\n",
    "        \n",
    "        t = pd.DataFrame(df_all.iloc[index][\"T\"])    \n",
    "        t.index=name_files\n",
    "        t.columns = [\"T\"]    \n",
    "\n",
    "        g = pd.DataFrame(df_all.iloc[index][\"G\"])\n",
    "        g.index=name_files\n",
    "        g.columns = [\"G\"]\n",
    "\n",
    "        c = pd.DataFrame(df_all.iloc[index][\"C\"])\n",
    "        c.index=name_files\n",
    "        c.columns = [\"C\"]\n",
    "\n",
    "        insertion = pd.DataFrame(df_all.iloc[index][\"+\"])\n",
    "        insertion.index=name_files\n",
    "        insertion.columns = [\"+\"]\n",
    "\n",
    "        deletion = pd.DataFrame(df_all.iloc[index][\"-\"])\n",
    "        deletion.index=name_files\n",
    "        deletion.columns = [\"-\"]\n",
    "\n",
    "        df_tmp = pd.concat([df_tmp, a, t, g, c, insertion, deletion], axis=1, sort=False)                \n",
    "        df_scaled = transform_df(df_tmp)\n",
    "        \n",
    "        df = pd.concat([df, df_scaled], axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, dirpath):\n",
    "    \n",
    "    nets = 0\n",
    "    list_models = []\n",
    "    y_probs = np.array(0)\n",
    "    df_activations = pd.DataFrame()    \n",
    "    dirpath = os.walk(dirpath)\n",
    "    load_model = tf.keras.models.load_model        \n",
    "    \n",
    "    for dirpath, dirnames, filenames in dirpath:\n",
    "        for filename in [f for f in filenames if f.endswith(\".model\")]:   \n",
    "            path_model = os.path.join(dirpath, filename)    \n",
    "            list_models.append(path_model)\n",
    "    \n",
    "    for i in trange(len(list_models)):                                                    \n",
    "            path_model = os.path.join(dirpath, filename)             \n",
    "            name_model = path_model.split('/')[-1]                   \n",
    "            train_model = load_model(path_model)            \n",
    "            #y_pred = train_model.predict_classes(X_test)\n",
    "            y_probs = y_probs + train_model.predict(X_test)                        \n",
    "            time.sleep(0.01)\n",
    "            nets += 1               \n",
    "    return y_probs/nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.-Preprocessing file: 01M1_HG.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "2.-Preprocessing file: 01M4_V.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "3.-Preprocessing file: ZH13.HG.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "4.-Preprocessing file: ZH16.V.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "5.-Preprocessing file: 01M1_H.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "6.-Preprocessing file: ZH.F16.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "7.-Preprocessing file: ZH13.H.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "8.-Preprocessing file: ZH13.S.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "9.-Preprocessing file: 01M1_S.fastq ...\n",
      "\tAligning with E. Coli K12 Genome\n",
      "\tGenerating pileup\n",
      "\tGenerating frequency table \n",
      "Reading files...\n",
      "Preprocessing dataframe...\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:01<00:00, 23.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"Samples/\"\n",
    "output_dir = \"output\"\n",
    "model_dir = \"Model/\"\n",
    "\n",
    "pos_file = \"pos_file.bed\"\n",
    "ref_genome = \"ref/Ecoli_K12_ref.fasta\"\n",
    "\n",
    "threads = \"16\"\n",
    "\n",
    "bases = [\"A\",\"T\",\"G\",\"C\",\"+\",\"-\"]\n",
    "\n",
    "alignment_dir = \"alignments\"\n",
    "pileup_dir = \"pileups\"\n",
    "frequency_dir = \"frequencies\"\n",
    "\n",
    "generate_tmp_folder(output_dir) \n",
    "generate_tmp_folder(output_dir+\"/\"+alignment_dir) \n",
    "generate_tmp_folder(output_dir+\"/\"+pileup_dir) \n",
    "generate_tmp_folder(output_dir+\"/\"+frequency_dir) \n",
    "\n",
    "dirpath = os.walk(dirpath)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for dirpath, dirnames, filenames in dirpath:\n",
    "    for filename in [f for f in filenames if f.endswith(\".fastq\")]:        \n",
    "        path_fastq_file = os.path.join(dirpath, filename)             \n",
    "        name_fastq_file = path_fastq_file.split('/')[-1]                                                \n",
    "        \n",
    "        sam_file = output_dir+\"/\"+alignment_dir+\"/\"+name_fastq_file+\".sam\"                    \n",
    "        bam_file = output_dir+\"/\"+alignment_dir+\"/\"+name_fastq_file+\".bam\"                    \n",
    "        mpileup = output_dir+\"/\"+pileup_dir+\"/\"+name_fastq_file+\".mpileup\"        \n",
    "        frequency_file = output_dir+\"/\"+frequency_dir+\"/\"+name_fastq_file+\".freq\" \n",
    "        \n",
    "        print(str(i)+\".-Preprocessing file: \"+name_fastq_file+\" ...\")                \n",
    "        print(\"\\tAligning with E. Coli K12 Genome\")        \n",
    "        run_bwa_mem(path_fastq_file, sam_file, threads, ref_genome)                            \n",
    "        cmd = \"samtools view -@ {} -bS {} | samtools sort -@ {} -m 2G -o {}\".format(threads, sam_file, threads, bam_file)                                \n",
    "        subprocess.call(cmd, shell=True)                                \n",
    "        cmd = \"samtools index -@ {} {}\".format(threads, bam_file)        \n",
    "        subprocess.call(cmd, shell=True)                    \n",
    "        print(\"\\tGenerating pileup\")        \n",
    "        cmd = \"samtools mpileup -d 100000 -l {} {} > {}\".format(pos_file, bam_file, mpileup )                        \n",
    "        subprocess.call(cmd, shell=True)        \n",
    "        mpileup = pd.read_table(mpileup, names=[\"chr\",\"pos\",\"ref\",\"reads\",\"seq\",\"qual\"])\n",
    "        print(\"\\tGenerating frequency table \")\n",
    "        df_freq_table = get_frequency_table(mpileup)        \n",
    "        df_freq_table.index.names = ['position']\n",
    "        df_freq_table.to_csv(frequency_file, sep=\"\\t\", index=True)                \n",
    "        i = i+1        \n",
    "X_test, name_files_test = process_frequencies(output_dir+\"/\"+frequency_dir)\n",
    "y_probs = predict(X_test.values, model_dir)\n",
    "df_probs = pd.DataFrame(y_probs,name_files_test)\n",
    "df_probs.columns = [\"Skin\", \"Oral\",\"Vagina\"]\n",
    "df_probs.to_csv(output_dir+\"/\"+\"predictions.csv\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
